{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import focal_loss\n",
    "import wandb\n",
    "from linformer import Linformer\n",
    "import random\n",
    "import matplotlib.pyplot as plt      \n",
    "import glob   \n",
    "from itertools import chain   \n",
    "from vit_pytorch.efficient import ViT   \n",
    "from tqdm.notebook import tqdm   \n",
    "from __future__ import print_function\n",
    "# import torch and related libraries \n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms   \n",
    "from torch.optim.lr_scheduler import StepLR   \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# CONSTS (might need changing)\n",
    "data_pth = '/l/users/aleksandr.matsun/stl_balanced/home/aleksandr.matsun/stl_datasets/stl_ds_balanced'\n",
    "model_name = 'vit'\n",
    "labels_train_path = 'labels_balanced_train.csv'\n",
    "labels_valid_path = 'labels_balanced_valid.csv'\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "#batch_size = 4\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "mask='111'\n",
    "checkpoint_pth = 'checkpoints_balanced_' + mask\n",
    "load_weights = False\n",
    "weights_pth = 'path/to/checkpoint.pth' # !!!!! WEIGHTS MUST BE WRAPPED IN DATAPARALLEL, OTHERWISE CRASHES !!!!!\n",
    "wandb_project_name = 'stl_' + mask\n",
    "\n",
    "\n",
    "# WANDB\n",
    "# wandb.init(project=wandb_project_name)\n",
    "# wandb.config = {\n",
    "#         'learning_rate': 0.0001,\n",
    "#         'epochs': 5,\n",
    "#         'batch_size': 256\n",
    "# }\n",
    "\n",
    "# DATASET\n",
    "\n",
    "class STLDataset(Dataset):\n",
    "    def __init__(self, data_dir, lbl_path, mask='111', transforms=None):\n",
    "        self.mask = mask\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.df = pd.read_csv(lbl_path, index_col=0)\n",
    "        self.instruments = list(self.df.columns[2:16])\n",
    "        \n",
    "        self.df['tools_present'] = self.df['tools_present'].apply(lambda x: x.strip(\"[']\").split(\"', '\"))\n",
    "        self.df['tools_present'] = self.df['tools_present'].apply(lambda x: [x[i] for i in range(3) if self.mask[i] == '1'])\n",
    "        for i in self.instruments:\n",
    "            self.df[i] = self.df['tools_present'].apply(lambda x: float(i in x))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_name = row['clip_name'] + '/' + row['frame']\n",
    "        #img_data = np.array(Image.open(self.img_dir + '/' + img_name).convert('RGB'), dtype='float32')\n",
    "        img_data = Image.open(self.data_dir + '/' + img_name)\n",
    "        if self.transforms:\n",
    "            img_data = self.transforms(img_data)\n",
    "            \n",
    "        return img_data, torch.from_numpy(self.df[self.instruments].iloc[index].values)\n",
    "\n",
    "transies = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((615, 900)),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.RandomErasing(p=0.2, value='random')\n",
    "])\n",
    "\n",
    "transies_v = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop((615, 900)),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "ds = STLDataset(data_dir=data_pth, \n",
    "                lbl_path=labels_train_path, \n",
    "                mask=mask,\n",
    "                transforms=transies)\n",
    "\n",
    "smol_ds = Subset(ds, np.arange(1000))\n",
    "\n",
    "ds_v = STLDataset(data_dir=data_pth, \n",
    "                lbl_path=labels_valid_path, \n",
    "                mask=mask,\n",
    "                transforms=transies_v)\n",
    "\n",
    "dl = DataLoader(smol_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_v = DataLoader(ds_v, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# MODEL\n",
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")\n",
    "print('=' * 10, 'preparing the model', '=' * 10)\n",
    "if model_name == 'resnet50':\n",
    "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    n_in_f = model.fc.in_features\n",
    "    model.fc = nn.Linear(n_in_f, 14)\n",
    "    model = DataParallel(module=model)\n",
    "elif(model_name == 'vit'):\n",
    "    model = ViT(\n",
    "    dim=128,\n",
    "    image_size=256,\n",
    "    patch_size=32,\n",
    "    num_classes=14,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ").to(device)\n",
    "model = DataParallel(module=model)\n",
    "\n",
    "# loading weights\n",
    "if load_weights:\n",
    "    model.load_state_dict(torch.load(weights_pth)) \n",
    "\n",
    "#model.to(device)\n",
    "\n",
    "criterion = focal_loss.sigmoid_focal_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "# TRAIN + VALID CYCLE\n",
    "for epoch in range(5):\n",
    "    print('epoch', epoch, 'started')\n",
    "    \n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    for i, (data_, target_) in enumerate(dl):\n",
    "        data_, target_ = data_, target_.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs, target_, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('loss:', loss.data)\n",
    "            wandb.log({'loss':loss})\n",
    "\n",
    "    model.eval()\n",
    "    preds = np.empty((0,14), float)\n",
    "    for i, (data_, target_) in enumerate(dl_v):\n",
    "        outputs = model(data_.to(device))\n",
    "        outputs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        preds = np.append(preds, outputs, axis=0)\n",
    "        if preds.shape[0] % 100 == 0:\n",
    "            print('eval iteration', preds.shape[0])\n",
    "    target = ds_v.df[ds_v.df.columns[2:16]].values\n",
    "    f1 = f1_score(target, preds > 0.5, average='macro')\n",
    "    auc = roc_auc_score(target, preds, average='macro')\n",
    "    print(f1, auc)\n",
    "    wandb.log({'val_f1_score':f1})\n",
    "    wandb.log({'val_auc':auc})\n",
    "        \n",
    "    print('epoch', epoch, 'done')\n",
    "    torch.save(model.state_dict(), checkpoint_pth + '/' + model_name + '_epoch' + str(epoch) + '_' + mask + '.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('miccai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f6cfd28a4e6a9bb65512d55507f896ccd85344506a5e0f58c152825f029472b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
